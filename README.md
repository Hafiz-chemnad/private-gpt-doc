# PrivateGPT Document Chatbot

This project is a private, local-first document chatbot built using FastAPI, integrating Large Language Models (LLMs) and embeddings for private document querying. It allows you to ingest your own documents and chat with them without sending your data to external services.

## Features

* **Local-First & Private:** All data processing, embeddings, and LLM inference happen locally on your machine.
* **Document Ingestion:** Upload and process various document types (e.g., `.txt`, `.pdf`, `.docx`) into a local vector database.
* **Retrieval Augmented Generation (RAG):** Answers are generated by retrieving relevant chunks from your ingested documents and feeding them to the LLM.
* **Chat Interface:** A simple web-based chat interface to interact with your documents.
* **Admin Panel:** Manage (list, delete, upload) your ingested documents.
* **Arabic Language Support:** Configured to handle Arabic documents and queries using a multilingual embedding model.
* **Modular Architecture:** Built with FastAPI for the API, Uvicorn for the server, and `ollama` for LLM integration.

## Technologies Used

* **Backend:** Python 3.x, FastAPI, Uvicorn
* **LLM Integration:** Ollama (for running `phi3:mini` locally)
* **Embeddings:** `sentence-transformers` (`intfloat/multilingual-e5-large` for multilingual support)
* **Vector Database:** ChromaDB
* **Document Loaders & Text Splitters:** `langchain`
* **Frontend:** HTML, CSS, JavaScript

## Local Setup Guide

Follow these steps to get the project up and running on your local machine.

### Prerequisites

* **Python 3.10+:** [Download Python](https://www.python.org/downloads/)
* **Git:** [Download Git for Windows](https://git-scm.com/download/win) (Choose Git Bash during installation)
* **Ollama:** [Download Ollama](https://ollama.com/download) and install it. This runs your local LLMs.

### Installation

1.  **Clone the Repository:**
    Open Git Bash (or Command Prompt/PowerShell) and navigate to the directory where you want to clone the project.
    ```bash
    git clone [https://github.com/Hafiz-chemnad/private-gpt-doc.git](https://github.com/Hafiz-chemnad/private-gpt-doc.git)
    cd private-gpt-doc
    ```

2.  **Create and Activate a Virtual Environment:**
    It's highly recommended to use a virtual environment to manage dependencies.
    ```bash
    python -m venv venv
    # On Windows (Git Bash/CMD/PowerShell):
    source venv/Scripts/activate
    # On Linux/macOS:
    # source venv/bin/activate
    ```
    Your terminal prompt should now show `(venv)` indicating the virtual environment is active.

3.  **Install Python Dependencies:**
    Install all required libraries from `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Install the LLM using Ollama:**
    Ensure Ollama is running in the background. Then, pull the `phi3:mini` model.
    ```bash
    ollama run phi3:mini
    # Wait for the download to complete, then you can type 'bye' and press enter to exit the Ollama chat.
    ```

5.  **Configure Models:**
    * Open `config.py` in your project's root directory.
    * Ensure the following lines are set for Arabic support and the `phi3` model:
        ```python
        # For embeddings
        EMBEDDINGS_MODEL_NAME = "intfloat/multilingual-e5-large"
        # For the LLM
        LLM_MODEL_NAME = "phi3:mini"
        # For Ollama LLM integration
        OLLAMA_BASE_URL = "http://localhost:11434"
        ```
    * These models will be automatically downloaded by the application on its first run if not already cached.

6.  **Clean up previous data (if any):**
    If you've run the project before with different models, it's crucial to delete the old database and model caches for a clean start with Arabic support.
    ```bash
    # Delete the local vector database
    rm -rf db/
    # Delete the source documents (optional, if you want a clean start)
    rm -rf source_documents/
    ```
    *You might also want to clear your Hugging Face cache for embedding models:*
    * Navigate to `C:\Users\YourUsername\.cache\huggingface\hub` (replace `YourUsername`).
    * Delete all the contents inside the `hub` directory. This forces a fresh download of `intfloat/multilingual-e5-large`.

### Running the Application

1.  **Start the FastAPI Server:**
    Make sure your virtual environment is activated.
    ```bash
    python api_server.py
    ```
    The server will start, and you'll see output like `Uvicorn running on http://127.0.0.1:8000`. The first time, it will download `intfloat/multilingual-e5-large`, which might take some time depending on your internet connection.

2.  **Access the Frontend:**
    Open your web browser and go to:
    * **Main Chat Interface:** `http://127.0.0.1:8000/`
    * **Admin Panel (Login required):** `http://127.0.0.1:8000/admin` (Default password is 'admin' for initial setup)

### Document Ingestion

1.  **Upload Documents:**
    * Go to the Admin Panel (`http://127.0.0.1:8000/admin`).
    * Log in (default password: `admin`).
    * Use the "Upload Document" section to upload your `.txt`, `.pdf`, `.docx` files.
    * The terminal running `api_server.py` will show the ingestion progress. Wait for `Ingestion complete!` message.

### Chatting with Your Documents (Arabic Support)

1.  **Go to the Chat Interface:**
    * Open `http://127.0.0.1:8000/`.
2.  **Ask Questions:**
    * Type your questions in the chat box. You can ask questions in **Arabic** relevant to your ingested Arabic documents.
    * Example (Arabic): `ما هي أهمية اللغة العربية؟` (What is the importance of the Arabic language?)

## Hosting (Advanced)

Hosting this project, especially with local LLMs and embedding models, requires significant RAM. While Oracle Cloud Free Tier offers generous ARM-based VMs (up to 24GB RAM), it requires a credit card for verification and involves manual server setup (Linux commands, Nginx, systemd, etc.).

For more information on hosting, refer to the detailed discussions on GitHub issues or community forums related to deploying Python/FastAPI applications with local AI models.

## Troubleshooting

* **"AttributeError: module 'bcrypt' has no attribute '__about__'":** This error might appear in your terminal logs during login but does not prevent successful login or core functionality. It can usually be ignored.
* **"NotImplementedError: Cannot copy out of meta tensor...":** This error during embeddings initialization should no longer occur if you followed the cache clearing steps. If it does, ensure your `torch` installation is compatible with your environment or re-clear the Hugging Face cache.
* **Models not downloading / Ingestion stuck:** Check your internet connection and ensure Ollama is running (`ollama run phi3:mini` should pull the model if not present).
* **"No Content Found" / Irrelevant Answers:** Ensure you have ingested documents relevant to your questions. Also, verify your `config.py` has the correct `EMBEDDINGS_MODEL_NAME` and `LLM_MODEL_NAME` after changing them.

---